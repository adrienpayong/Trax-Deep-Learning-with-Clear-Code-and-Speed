# Trax-Deep Learning with Clear Code and Speed
Google Brain developed Tensor2Tensor (T2T) to make deep learning development easier. T2T is an extension of TensorFlow and contains a
library of deep learning models that contains many Transformer examples. Though T2T was a good start, Google Brain produced Trax, an end-to-end
deep learning library. Trax contains a transformer model that can be applied to translations. The Google Brain team presently maintains Trax.
In this [notebook](https://github.com/adrienpayong/Trax-Deep-Learning-with-Clear-Code-and-Speed/blob/main/Trax_%E2%80%94_Deep_Learning_with_Clear_Code_and_Speed.ipynb), we will focus on the minimum functions to initialize the English-German problem described by Vaswani et al. (2017) to illustrate the transformer's performance.
[Trax-Deep Learning with Clear Code and Speed](https://github.com/google/trax)

